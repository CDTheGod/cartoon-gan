{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartoonGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61gxheM2om8l",
        "colab_type": "text"
      },
      "source": [
        "# CartoonGAN\n",
        "\n",
        "This notebook contains the implementation of the cartoon GAN model. It is implemented with PyTorch. See README [here](https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/README.md) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-GRcOxmm-8_",
        "colab_type": "text"
      },
      "source": [
        "## Generate dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHbkMli7BFc",
        "colab_type": "text"
      },
      "source": [
        "## Transfer data via google drive\n",
        "- all image data in this notebook is expected to be zipped to files on local computer as described in README of this project [here](https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/README.md)\n",
        "- create folder `cartoonGAN` in `My Drive` in google drive\n",
        "- copy .zip-files `coco.zip`, `safebooru.zip` and `safebooru_smoothed.zip` to google drive `My Drive`/`cartoonGAN`\n",
        "- mount google drive in this notebook by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WzcH9ef4dc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pee_KkYm_W1",
        "colab_type": "text"
      },
      "source": [
        "### cartoons images\n",
        "\n",
        "- cartoon images are located in file `content/data/My Drive/cartoonGAN/safebooru.zip` of this notebook\n",
        "- extract images and place in folder `cartoons` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b342SwfIR1ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir cartoons\n",
        "!mkdir cartoons/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/safebooru.zip -d cartoons/1/ #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPj9lzdNmvir",
        "colab_type": "text"
      },
      "source": [
        "##### data-loader\n",
        "\n",
        "As mentioned in the paper, the used image size is 256x256 pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebY_JkCBVuyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "import math\n",
        "\n",
        "image_size = 256\n",
        "batch_size = 16\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "cartoon_dataset = ImageFolder('cartoons/', transformer)\n",
        "len_training_set = math.floor(len(cartoon_dataset) * 0.9)\n",
        "len_valid_set = len(cartoon_dataset) - len_training_set\n",
        "\n",
        "training_set, _ = random_split(cartoon_dataset, (len_training_set, len_valid_set))\n",
        "cartoon_image_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0kxqmDopf0P",
        "colab_type": "text"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYapuAsuoOrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_sample_image(dataloader):\n",
        "  iterator = iter(dataloader)\n",
        "  sample_batch, _ = iterator.next()\n",
        "  first_sample_image_of_batch = sample_batch[0]\n",
        "  print(first_sample_image_of_batch.size())\n",
        "  print(\"Current range: {} to {}\".format(first_sample_image_of_batch.min(), first_sample_image_of_batch.max()))\n",
        "  plt.imshow(np.transpose(first_sample_image_of_batch.numpy(), (1, 2, 0)))\n",
        "\n",
        "show_sample_image(cartoon_image_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_PUKl2PHRYYP"
      },
      "source": [
        "### edge-smoothed cartoons images\n",
        "\n",
        "- edge-smoothed cartoon images are located in file `content/data/My Drive/cartoonGAN/safebooru_smoothed.zip` of this notebook\n",
        "- extract images and place in folder `cartoons_smoothed` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQVv5Q-IRo1_",
        "colab": {}
      },
      "source": [
        "!mkdir cartoons_smoothed\n",
        "!mkdir cartoons_smoothed/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/safebooru_smoothed.zip -d cartoons_smoothed/1/ #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hqBoaA8tSZh8"
      },
      "source": [
        "##### data-loader\n",
        "\n",
        "same configuration as cartoon data loader above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8aepIFEfSiZ3",
        "colab": {}
      },
      "source": [
        "smoothed_cartoon_dataset = ImageFolder('cartoons_smoothed/', transformer)\n",
        "len_training_set = math.floor(len(smoothed_cartoon_dataset) * 0.9)\n",
        "len_valid_set = len(smoothed_cartoon_dataset) - len_training_set\n",
        "training_set, _ = random_split(smoothed_cartoon_dataset, (len_training_set, len_valid_set))\n",
        "smoothed_cartoon_image_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6IM7YUkgSE96"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z_qeh0BqSE-B",
        "colab": {}
      },
      "source": [
        "show_sample_image(smoothed_cartoon_image_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gL-nH1Q31vd",
        "colab_type": "text"
      },
      "source": [
        "### photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PlLTrVWKtqS",
        "colab_type": "text"
      },
      "source": [
        "- photos are located in file `content/data/My Drive/cartoonGAN/coco.zip` of this notebook\n",
        "- extract images and place in folder `photos` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQYRvLUWrPO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir photos\n",
        "!mkdir photos/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/coco.zip -d photos/1 #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvscdHFdNLbf",
        "colab_type": "text"
      },
      "source": [
        "##### data-loader\n",
        "same configuration as cartoon data loader above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Db-YnYWNRIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photo_dataset = ImageFolder('photos/', transformer)\n",
        "len_training_set = math.floor(len(photo_dataset) * 0.9)\n",
        "len_valid_set = len(photo_dataset) - len_training_set\n",
        "training_set, validation_set = random_split(photo_dataset, (len_training_set, len_valid_set))\n",
        "photo_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)\n",
        "photo_dataloader_valid = DataLoader(validation_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWreH4XwNplc",
        "colab_type": "text"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIO325gNNqIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_sample_image(photo_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp40ZnyTc99G",
        "colab_type": "text"
      },
      "source": [
        "##Define model\n",
        "\n",
        "The information about the model structure is given in the paper.\n",
        "\n",
        "For the `zero padding` of the convolutional layers, I use the following formula:\n",
        "\n",
        "$$Height x Width_{output} = \\frac{HeightxWidth_{input} - kernel size + 2 padding}{stride} + 1$$\n",
        "\n",
        "e.g:\n",
        "\n",
        "- `conv_1` layer of generator: $HxW$ should stay the same as input size, which is 256x256 and `stride = 1`\n",
        "\n",
        "$$256 = \\frac{256-7+2padding}{1}+1$$\n",
        "\n",
        "$$padding = 3$$\n",
        "\n",
        "In case of a fraction as a result, I choose to ceil:\n",
        "\n",
        "- `conv_2` layer of generator: $\\frac{H}{2} x \\frac{W}{2}$ is output with `stride=2`\n",
        "\n",
        "$$128 = \\frac{256-3+2padding}{2}+1$$\n",
        "\n",
        "$$padding= \\frac{1}{2} \\Rightarrow padding=1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mW5STOdDyn",
        "colab_type": "text"
      },
      "source": [
        "###Generator\n",
        "\n",
        "In the up-convolutional part of the paper, two layers (conv_6 and conv_8 in my implementation) have a stride of 0.5. As pytorchs `conv2D()` does not allow floating point stride, I use a stride of 1 in both cases. Therefore even for the padding calculation I go with a stride of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2EmDnsucENx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv_1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.norm_1 = nn.BatchNorm2d(256)\n",
        "    self.norm_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.norm_2(self.conv_2(F.relu(self.norm_1(self.conv_1(x)))))\n",
        "    return output + x #ES\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Generator, self).__init__()\n",
        "      self.conv_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
        "      self.norm_1 = nn.BatchNorm2d(64)\n",
        "      \n",
        "      # down-convolution #\n",
        "      self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "      self.conv_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_2 = nn.BatchNorm2d(128)\n",
        "      \n",
        "      self.conv_4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
        "      self.conv_5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_3 = nn.BatchNorm2d(256)\n",
        "      \n",
        "      # residual blocks #\n",
        "      residualBlocks = []\n",
        "      for l in range(8):\n",
        "        residualBlocks.append(ResidualBlock())\n",
        "      self.res = nn.Sequential(*residualBlocks)\n",
        "      \n",
        "      # up-convolution #\n",
        "      self.conv_6 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=33)\n",
        "      self.conv_7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_4 = nn.BatchNorm2d(128)\n",
        "\n",
        "      self.conv_8 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=65)\n",
        "      self.conv_9 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_5 = nn.BatchNorm2d(64)\n",
        "      \n",
        "      self.conv_10 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.norm_1(self.conv_1(x)))\n",
        "      \n",
        "      x = F.relu(self.norm_2(self.conv_3(self.conv_2(x))))\n",
        "      x = F.relu(self.norm_3(self.conv_5(self.conv_4(x))))\n",
        "      \n",
        "      x = self.res(x)\n",
        "\n",
        "      x = F.relu(self.norm_4(self.conv_7(self.conv_6(x))))\n",
        "      x = F.relu(self.norm_5(self.conv_9(self.conv_8(x))))\n",
        "      \n",
        "      x = self.conv_10(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATqXFxcpz0eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator()\n",
        "print(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtvIkqrVy34",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxJ2dldGz5Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "     super(Discriminator, self).__init__()\n",
        "     self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "      \n",
        "     self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "     self.conv_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_1 = nn.BatchNorm2d(128)\n",
        "      \n",
        "     self.conv_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "     self.conv_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_2 = nn.BatchNorm2d(256)\n",
        "    \n",
        "     self.conv_6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_3 = nn.BatchNorm2d(256)\n",
        "    \n",
        "     self.conv_7 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if(torch.isnan(x).any()):\n",
        "      print(\"WARN nans 0\")\n",
        "    x = F.leaky_relu(self.conv_1(x))\n",
        "    if(torch.isnan(x).any()):\n",
        "      print(\"WARN nans 01\")\n",
        "    x = F.leaky_relu(self.norm_1(self.conv_3(F.leaky_relu(self.conv_2(x)))), negative_slope=0.2)\n",
        "    if(torch.isnan(x).any()):\n",
        "      print(\"WARN nans 02\")\n",
        "    x = F.leaky_relu(self.norm_2(self.conv_5(F.leaky_relu(self.conv_4(x)))), negative_slope=0.2)\n",
        "    if(torch.isnan(x).any()):\n",
        "      print(\"WARN nans 03\")\n",
        "    x = F.leaky_relu(self.norm_3(self.conv_6(x)), negative_slope=0.2)\n",
        "    if(torch.isnan(x).any()):\n",
        "      print(\"WARN nans 04\")\n",
        "    x = self.conv_7(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI6tJdLmlUHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = Discriminator()\n",
        "print(D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwVzSn9nB1jL",
        "colab_type": "text"
      },
      "source": [
        "### move to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IMrqAe1lVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  G.cuda()\n",
        "  D.cuda()\n",
        "  print(\"Train on GPU. Models moved.\")\n",
        "else:\n",
        "  print(\"No cuda available\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIU4f6DiCldX",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "$\\mathcal{L}(G, D) = \\mathcal{L}_{adv}(G, D) + ω\\mathcal{L}_{con}(G, D)$ with ω = 10\n",
        "\n",
        "This loss is used to train the discriminator and the generator. In the adversarial part, the discriminator tries to classify the generated images as fakes.\n",
        "\n",
        "During the generator training, the generator tries to minimize the classifications, where the discriminator classifies the generated image as fake. The generator has only affect on the parts of the formula where $G()$ is involved, so the generator tries to minimize this part. Additionally, the loss is not directly calculated from the generator output, but from the discriminator output. Due to the fact that the generator output is the input for the discriminator output in the generator training, the generator is in the chain of the backpropagation, when the loss from the discriminator output is backprogagated all the way back through the discriminator model and generator model to the photo image input data [1], [2].\n",
        "\n",
        "[1] https://developers.google.com/machine-learning/gan/generator\n",
        "\n",
        "[2] https://towardsdatascience.com/only-numpy-implementing-gan-general-adversarial-networks-and-adam-optimizer-using-numpy-with-2a7e4e032021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXuzfGx4Ctlc",
        "colab_type": "text"
      },
      "source": [
        "### Adversarial loss\n",
        "\n",
        "The adversarial loss  $\\mathcal{L}_{adv}(G, D)$ which drives the generator to transform photo to comic style of the image. Its value indicates if the output looks like a cartoon image or not. The paper highlights, that a characteristic part of cartoons images are the clear edges, which are a small detail of the image, must be preserved to generate clear edges in the result. In the paper, this is solved by training not only with cartoon images but additionaly by training with the same cartoon images with smoothed edges so that the discriminator can distinguish between clear and smooth edges. For achieving this the authors define the edge-promoting adversarial loss function:\n",
        "\n",
        "$\\mathcal{L}_{adv}(G, D) = \\mathbb{E}_{ci∼S_{data}(c)}[log D(c_i)]\n",
        "+ \\mathbb{E}_{ej∼S_{data}(e)}[log(1 − D(e_j))]\n",
        "+ \\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$\n",
        "\n",
        "- for the discriminator, this is the formula for the loss function, because output of the Discriminator plays no role within the content loss part of the loss function.\n",
        "\n",
        "- for the initialization phase of the generator, this part of the formula is not used as described in the paper.\n",
        "\n",
        "- for the training phase of the generator, only the part of the formula is used within the generator loss function, which the generator can affect: $\\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEloi8fChEfH",
        "colab_type": "text"
      },
      "source": [
        "### Content loss\n",
        "The content loss $ω\\mathcal{L}_{con}(G, D)$ which preserves the semantic content during transformation. To calculate this, in the paper the high-level feature maps of the VGG network is used, in particular the layer ($l$) `conv4_4`. The output of the layer $l$ for the original photo is subtracted from the output of the layer $l$ of the generated image. The result is regularized using the $\\mathcal{L_1}$ spare regularization ($||...||_1$):\n",
        "\n",
        "$\\mathcal{L}_{con}(G, D)= \\mathbb{E}_{pi~S_{data}(p)}[||VGG_l(G(p_i))-VGG_l(p_i)||_1]$\n",
        "\n",
        "This part of the formula plays a role in the loss function for the generator, not for the discriminator, because only the generator is used within this formula.\n",
        "\n",
        "More info about $\\mathcal{L_1}$ regularization:\n",
        "\n",
        "https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a\n",
        "\n",
        "https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGwo1goXCW_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "print(vgg16)\n",
        "\n",
        "# due VGG16 has 5 pooling-layer, I assume conv4_4 is the 4th pooling layer\n",
        "# (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "feature_extractor = vgg16.features[:24]\n",
        "for param in feature_extractor.parameters():\n",
        "  param.require_grad = False\n",
        "\n",
        "print(feature_extractor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McDAYOutQ0Xh",
        "colab_type": "text"
      },
      "source": [
        "### Three loss functions\n",
        "\n",
        "- discriminator loss\n",
        "- generator initialization phase loss\n",
        "- generator loss\n",
        "\n",
        "In order to return a single scalar-value as result of the loss function, I choose to calculate the mean as last step of the loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjkXoxfaRI5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "class DiscriminatorLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(DiscriminatorLoss, self).__init__()\n",
        "\n",
        "  def forward(self, discriminator_output_of_cartoon_input,\n",
        "              discriminator_output_of_cartoon_smoothed_input,\n",
        "              discriminator_output_of_generated_image_input):\n",
        "\n",
        "    return self._adversarial_loss(discriminator_output_of_cartoon_input,\n",
        "                     discriminator_output_of_cartoon_smoothed_input,\n",
        "                     discriminator_output_of_generated_image_input)\n",
        "\n",
        "  def _adversarial_loss(self, discriminator_output_of_cartoon_input,\n",
        "                     discriminator_output_of_cartoon_smoothed_input,\n",
        "                     discriminator_output_of_generated_image_input):\n",
        "    if(torch.isnan(discriminator_output_of_cartoon_input).any()):\n",
        "      print(\"WARN discriminator_output_of_cartoon_input contains nans\")\n",
        "    \n",
        "    if(torch.isnan(discriminator_output_of_cartoon_smoothed_input).any()):\n",
        "      print(\"WARN discriminator_output_of_cartoon_smoothed_input contains nans\")\n",
        "    \n",
        "    if(torch.isnan(discriminator_output_of_generated_image_input).any()):\n",
        "      print(\"Warn discriminator_output_of_generated_image_input contains nans\")\n",
        "      \n",
        "    result = torch.log(torch.abs(discriminator_output_of_cartoon_input)) +\\\n",
        "             torch.log(torch.abs(1 - discriminator_output_of_cartoon_smoothed_input)) +\\\n",
        "             torch.log(torch.abs(1 - discriminator_output_of_generated_image_input))\n",
        "\n",
        "    return torch.mean(result **2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWkjEVr78Oqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorInitPhaseLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(GeneratorInitPhaseLoss, self).__init__()\n",
        "      vgg16 = models.vgg16(pretrained=True)\n",
        "      if torch.cuda.is_available():\n",
        "        vgg16.cuda()\n",
        "      self.feature_extractor = vgg16.features[:24]\n",
        "      for param in self.feature_extractor.parameters():\n",
        "        param.require_grad = False\n",
        "\n",
        "  def forward(self, generator_input, generator_output):\n",
        "    return self._content_loss(generator_input, generator_output)\n",
        "\n",
        "  def _content_loss(self, generator_input, generator_output):\n",
        "    return (self.feature_extractor(generator_output) - self.feature_extractor(generator_input)).norm(p=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-JzisW-65IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(GeneratorLoss, self).__init__()\n",
        "      self.w = 10\n",
        "      vgg16 = models.vgg16(pretrained=True)\n",
        "      if torch.cuda.is_available():\n",
        "        vgg16.cuda()\n",
        "      self.feature_extractor = vgg16.features[:24]\n",
        "      for param in self.feature_extractor.parameters():\n",
        "        param.require_grad = False\n",
        "\n",
        "  def forward(self, discriminator_output_of_generated_image_input,\n",
        "              generator_input,\n",
        "              generator_output):\n",
        "    return self._adversarial_loss_generator_part_only(discriminator_output_of_generated_image_input) +\\\n",
        "        self.w * self._content_loss(generator_input, generator_output)\n",
        "\n",
        "  def _adversarial_loss_generator_part_only(self, discriminator_output_of_generated_image_input):\n",
        "    return torch.mean(torch.log(torch.abs(discriminator_output_of_generated_image_input)) **2)\n",
        "\n",
        "  def _content_loss(self, generator_input, generator_output):\n",
        "    return (self.feature_extractor(generator_output) - self.feature_extractor(generator_input)).norm(p=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAlrFRkH2WaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminatorLoss = DiscriminatorLoss()\n",
        "generatorInitPhaseLoss = GeneratorInitPhaseLoss()\n",
        "generatorLoss = GeneratorLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF8stH-X3e7j",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer\n",
        "In the paper, the used optimizer is not mentioned, I decide to choose adam.\n",
        "\n",
        "For hyperparameter-tuning, I decided to go with the same parameters mentioned in the DCGAN-paper[1].\n",
        "\n",
        "[1] https://arxiv.org/pdf/1511.06434.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwrrY1z34WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
        "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6weYKGsHB-Vo",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "To make training resumeable, I save some checkpoints and load them, if existing, before run the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9b23aS94msx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/checkpoints/\n",
        "!mkdir /content/data/My\\ Drive/cartoonGAN/checkpoints/\n",
        "# if this is a resume, load existing checkpoints...if not, folder will be empty\n",
        "!cp /content/data/My\\ Drive/cartoonGAN/checkpoints/* /content/checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoxZv-93DEXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(_num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses):\n",
        "  init_epochs = 10\n",
        "  print_every = 100\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(_num_epochs - epochs_already_done):\n",
        "    epoch = epoch + epochs_already_done\n",
        "\n",
        "    for batch_index, ((photo_images, _), (smoothed_cartoon_images, _), (cartoon_images, _)) in enumerate(zip(photo_dataloader_train, smoothed_cartoon_image_dataloader_train, cartoon_image_dataloader_train)):\n",
        "      # try to find nan error\n",
        "      if(torch.isnan(photo_images).any()):\n",
        "        print(\"WARN photo images are corrupt\")\n",
        "        continue\n",
        "      if(torch.isnan(cartoon_images).any()):\n",
        "        print(\"WARN cartoon images are corrupt\")\n",
        "        continue\n",
        "      if(torch.isnan(smoothed_cartoon_images).any()):\n",
        "        print(\"WARN smoothed cartoon images are corrupt\")\n",
        "        continue\n",
        "\n",
        "      batch_size = photo_images.size(0)\n",
        "      if torch.cuda.is_available():\n",
        "        photo_images = photo_images.cuda()\n",
        "        smoothed_cartoon_images = smoothed_cartoon_images.cuda()\n",
        "        cartoon_images = cartoon_images.cuda()\n",
        "        if(torch.isnan(photo_images).any()):\n",
        "          print(\"WARN photo images are corrupt gpu\")\n",
        "          continue\n",
        "        if(torch.isnan(cartoon_images).any()):\n",
        "          print(\"WARN cartoon images are corrupt gpu\")\n",
        "          continue\n",
        "        if(torch.isnan(smoothed_cartoon_images).any()):\n",
        "          print(\"WARN smoothed cartoon images are corrupt gpu\")\n",
        "          continue\n",
        "\n",
        "      # train the discriminator\n",
        "      d_optimizer.zero_grad()\n",
        "\n",
        "      discriminator_output_of_cartoon_input = D(cartoon_images)\n",
        "      discriminator_output_of_cartoon_smoothed_input = D(smoothed_cartoon_images)\n",
        "      discriminator_output_of_generated_image_input = D(G(photo_images))\n",
        "\n",
        "      d_loss = discriminatorLoss(discriminator_output_of_cartoon_input,\n",
        "                            discriminator_output_of_cartoon_smoothed_input,\n",
        "                            discriminator_output_of_generated_image_input)\n",
        "\n",
        "      d_loss.backward()\n",
        "      d_optimizer.step()\n",
        "\n",
        "      # train the generator\n",
        "      g_optimizer.zero_grad()\n",
        "\n",
        "      generator_output = G(photo_images)\n",
        "      discriminator_output_of_generated_image_input = D(generator_output)\n",
        "\n",
        "      if epoch < init_epochs:\n",
        "        # init\n",
        "        g_loss = generatorInitPhaseLoss(photo_images, generator_output)\n",
        "      else:\n",
        "        # train\n",
        "        g_loss = generatorLoss(discriminator_output_of_generated_image_input,\n",
        "                             photo_images,\n",
        "                             generator_output)\n",
        "\n",
        "      g_loss.backward()\n",
        "      g_optimizer.step()\n",
        "\n",
        "      if(math.isnan(d_loss.item()) or math.isnan(g_loss.item())):\n",
        "        print(\"ERROR losses contain nans\")\n",
        "        break\n",
        "\n",
        "      if batch_index % print_every == 0:\n",
        "        losses.append((d_loss.item(), g_loss.item()))\n",
        "        now = time.time()\n",
        "        current_run_time = now - start_time\n",
        "        start_time = now\n",
        "        print(\"Epoch {}/{} | d_loss {:6.4f} | g_loss {:6.4f} | time {:2.0f}s | total no. of losses {}\".format(epoch+1, _num_epochs, d_loss.item(), g_loss.item(), current_run_time, len(losses)))\n",
        "    \n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "      D.eval()\n",
        "      G.eval()\n",
        "\n",
        "      for batch_index, (photo_images, _) in enumerate(photo_dataloader_valid):\n",
        "        if torch.cuda.is_available():\n",
        "          photo_images = photo_images.cuda()\n",
        "\n",
        "        generator_output = G(photo_images)\n",
        "        discriminator_output_of_generated_image_input = D(generator_output)\n",
        "        g_valid_loss = generatorLoss(discriminator_output_of_generated_image_input, photo_images, generator_output)\n",
        "\n",
        "        if batch_index % print_every == 0:\n",
        "          validation_losses.append(g_valid_loss.item())\n",
        "          now = time.time()\n",
        "          current_run_time = now - start_time\n",
        "          start_time = now\n",
        "          print(\"Epoch {}/{} | validation loss {:6.4f} | time {:2.0f}s | total no. of losses {}\".format(epoch+1, _num_epochs, g_valid_loss.item(), current_run_time, len(validation_losses)))\n",
        "\n",
        "    D.train()\n",
        "    G.train()\n",
        "\n",
        "    if(g_valid_loss.item() < best_valid_loss):\n",
        "      print(\"Generator loss improved from {} to {}\".format(best_valid_loss, g_valid_loss.item()))\n",
        "      best_valid_loss = g_valid_loss.item()\n",
        "\n",
        "    # save checkpoint\n",
        "    checkpoint = {'g_valid_loss': g_valid_loss.item(),\n",
        "                   'best_valid_loss': best_valid_loss,\n",
        "                   'losses': losses,\n",
        "                   'validation_losses': validation_losses,\n",
        "                   'last_epoch': epoch,\n",
        "                   'd_state_dict': D.state_dict(),\n",
        "                   'g_state_dict': G.state_dict(),\n",
        "                   'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                   'g_optimizer_state_dict': g_optimizer.state_dict()\n",
        "                 }\n",
        "    print(\"Save checkpoint for validation loss of {}\".format(g_valid_loss.item()))\n",
        "    torch.save(checkpoint, checkpoint_dir + '/checkpoint_epoch_{:03d}.pth'.format(epoch+1))\n",
        "    if(best_valid_loss == g_valid_loss.item()):\n",
        "      print(\"Overwrite best checkpoint\")\n",
        "      torch.save(checkpoint, checkpoint_dir + '/best_checkpoint.pth')\n",
        "\n",
        "  return losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXRAI5X_h2M",
        "colab_type": "text"
      },
      "source": [
        "Save checkpoints to google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWrCNWrMEwWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/checkpoints/* /content/data/My\\ Drive/cartoonGAN/checkpoints/ # -n?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P6_f7NTDXyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6bfa7f8-c774-448e-bdc3-00aa12ee9c6e"
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "checkpoint_dir = '/content/checkpoints'\n",
        "checkpoints = listdir(checkpoint_dir)\n",
        "num_epochs = 200 + 10 # training + init phase\n",
        "epochs_already_done = 0\n",
        "best_valid_loss = math.inf\n",
        "losses = []\n",
        "validation_losses = []\n",
        "\n",
        "if(len(checkpoints) > 0):\n",
        "  last_checkpoint = sorted(checkpoints)[-1]\n",
        "  checkpoint = torch.load(checkpoint_dir + '/' + last_checkpoint)\n",
        "  best_valid_loss = checkpoint['best_valid_loss']\n",
        "  epochs_already_done = checkpoint['last_epoch']\n",
        "  losses = checkpoint['losses']\n",
        "  validation_losses = checkpoint['validation_losses']\n",
        "  \n",
        "  D.load_state_dict(checkpoint['d_state_dict'])\n",
        "  G.load_state_dict(checkpoint['g_state_dict'])\n",
        "  d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "  g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "  print('Load checkpoint {} with g_valid_loss {}, best_valid_loss {}, {} epochs and total no of losses {}'.format(last_checkpoint, checkpoint['g_valid_loss'], best_valid_loss, epochs_already_done, len(losses)))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load checkpoint checkpoint_epoch_030.pth with g_valid_loss 9285884.0, best_valid_loss 8323977.0, 29 epochs and total no of losses 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppzLtQGwBBAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a14287da-df78-4b12-f1c1-93d8cc3cd5ed"
      },
      "source": [
        "losses, validation_losses = train(num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/210 | d_loss 7.8801 | g_loss 11631521.0000 | time  2s | total no. of losses 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VxBiLKImhYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure.format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_losses = [x[0] for x in losses]\n",
        "g_losses = [x[1] for x in losses]\n",
        "plt.plot(d_losses, label='Discriminator training loss')\n",
        "plt.plot(g_losses, label='Generator training loss')\n",
        "plt.plot(validation_losses, label='Generator validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ4wLFRfr0dS",
        "colab_type": "text"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4bnUSksAiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load('/content/checkpoint.pth')\n",
        "G_inference = Generator()\n",
        "G_inference.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1njcrDnS_Me",
        "colab_type": "text"
      },
      "source": [
        "## Notes/next steps\n",
        "- alternative lib for image processing: https://github.com/albu/albumentations\n",
        "- figure out which variant of VGG to use (VGG-16?), and if the pre-training in the referenced paper is the same as the pre-trained pytorch version\n",
        "- do I use the correct `log` in adversarial loss?\n",
        "- do I use correct normalization-method in content loss\n",
        "- hyperparameter tuning with tune\n",
        "- in which order is the discriminator trained regarding photos, cartoons with smoothed edges and then genereated images?\n",
        "- evaluate result with existing model http://cg.cs.tsinghua.edu.cn/people/~Yongjin/CartoonGAN-Models.rar ?\n",
        "- did I split the loss function correctly for the D and G model, and content loss only for G?\n",
        "- plot results directly from vars via method\n",
        "- is choosing mean as last step within loss functions correct?\n",
        "- is choosing abs() to transform generator/discriminator output to positive value for loss function correct? (due to negative input for log() produces NaN)\n",
        "- d_loss, (g_loss?) produces negtive values by only using mean(), is it correct to square the result before mean?\n",
        "- in the paper 6.000 photo images and 2.000 - 4.000 cartoon images are used for training, how is this done with unbalanced datasets?\n",
        "- is batch_size of 16 correct? Tried 32 before, but got CUDA OOM\n",
        "- for image downloader: catch exception if image is truncated/check if zipping adds additional folder within .zip in create_smoothed_images.py\n",
        "- validation loss within init phase is not calculated with init-loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o46NGauRrEcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/checkpoints/checkpoint_epoch_031.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjWcmyFjp_sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/checkpoints/checkpoint_epoch_027.pth /content/data/My\\ Drive/cartoonGAN/checkpoint_epoch_027.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUUiJBAjqea9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}