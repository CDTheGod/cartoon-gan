{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartoonGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61gxheM2om8l",
        "colab_type": "text"
      },
      "source": [
        "# CartoonGAN\n",
        "\n",
        "This notebook contains the implementation of the cartoon GAN model. It is implemented with PyTorch. See README [here](https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/README.md) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-GRcOxmm-8_",
        "colab_type": "text"
      },
      "source": [
        "## Generate dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHbkMli7BFc",
        "colab_type": "text"
      },
      "source": [
        "## Transfer data via google drive\n",
        "- all image data in this notebook is expected to be zipped to files on local computer as described in README of this project [here](https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/README.md)\n",
        "- create folder `cartoonGAN` in `My Drive` in google drive\n",
        "- copy .zip-files `coco.zip`, `safebooru.zip` and `safebooru_smoothed.zip` to google drive `My Drive`/`cartoonGAN`\n",
        "- mount google drive in this notebook by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WzcH9ef4dc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pee_KkYm_W1",
        "colab_type": "text"
      },
      "source": [
        "### cartoons images\n",
        "\n",
        "- cartoon images are located in file `content/data/My Drive/cartoonGAN/safebooru.zip` of this notebook\n",
        "- extract images and place in folder `cartoons` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b342SwfIR1ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir cartoons\n",
        "!mkdir cartoons/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/safebooru.zip -d cartoons/1/ #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPj9lzdNmvir",
        "colab_type": "text"
      },
      "source": [
        "##### data-loader\n",
        "\n",
        "As mentioned in the paper, the used image size is 256x256 pixel.\n",
        "\n",
        "The Generator uses relu as activation function, which generates values in [0.0, 1.0]. As the ToTensor()-method changes the range of the input image from RGB [0, 255] to [0.0, 1.0], we get the same range for all images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebY_JkCBVuyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "import math\n",
        "\n",
        "image_size = 256\n",
        "batch_size = 16\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor() # ToTensor() changes the range of the values from [0, 255] to [0.0, 1.0]\n",
        "])\n",
        "\n",
        "cartoon_dataset = ImageFolder('cartoons/', transformer)\n",
        "len_training_set = math.floor(len(cartoon_dataset) * 0.9)\n",
        "len_valid_set = len(cartoon_dataset) - len_training_set\n",
        "\n",
        "training_set, _ = random_split(cartoon_dataset, (len_training_set, len_valid_set))\n",
        "cartoon_image_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0kxqmDopf0P",
        "colab_type": "text"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYapuAsuoOrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_sample_image(dataloader):\n",
        "  iterator = iter(dataloader)\n",
        "  sample_batch, _ = iterator.next()\n",
        "  first_sample_image_of_batch = sample_batch[0]\n",
        "  print(first_sample_image_of_batch.size())\n",
        "  print(\"Current range: {} to {}\".format(first_sample_image_of_batch.min(), first_sample_image_of_batch.max()))\n",
        "  plt.imshow(np.transpose(first_sample_image_of_batch.numpy(), (1, 2, 0)))\n",
        "\n",
        "show_sample_image(cartoon_image_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_PUKl2PHRYYP"
      },
      "source": [
        "### edge-smoothed cartoons images\n",
        "\n",
        "- edge-smoothed cartoon images are located in file `content/data/My Drive/cartoonGAN/safebooru_smoothed.zip` of this notebook\n",
        "- extract images and place in folder `cartoons_smoothed` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQVv5Q-IRo1_",
        "colab": {}
      },
      "source": [
        "!mkdir cartoons_smoothed\n",
        "!mkdir cartoons_smoothed/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/safebooru_smoothed.zip -d cartoons_smoothed/1/ #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hqBoaA8tSZh8"
      },
      "source": [
        "##### data-loader\n",
        "\n",
        "same configuration as cartoon data loader above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8aepIFEfSiZ3",
        "colab": {}
      },
      "source": [
        "smoothed_cartoon_dataset = ImageFolder('cartoons_smoothed/', transformer)\n",
        "len_training_set = math.floor(len(smoothed_cartoon_dataset) * 0.9)\n",
        "len_valid_set = len(smoothed_cartoon_dataset) - len_training_set\n",
        "training_set, _ = random_split(smoothed_cartoon_dataset, (len_training_set, len_valid_set))\n",
        "smoothed_cartoon_image_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6IM7YUkgSE96"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z_qeh0BqSE-B",
        "colab": {}
      },
      "source": [
        "show_sample_image(smoothed_cartoon_image_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gL-nH1Q31vd",
        "colab_type": "text"
      },
      "source": [
        "### photos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PlLTrVWKtqS",
        "colab_type": "text"
      },
      "source": [
        "- photos are located in file `content/data/My Drive/cartoonGAN/coco.zip` of this notebook\n",
        "- extract images and place in folder `photos` by executing cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQYRvLUWrPO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir photos\n",
        "!mkdir photos/1\n",
        "!unzip -n -q /content/data/My\\ Drive/cartoonGAN/coco.zip -d photos/1 #extract to subfolder due to DataLoader needs subdirectories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvscdHFdNLbf",
        "colab_type": "text"
      },
      "source": [
        "##### data-loader\n",
        "same configuration as cartoon data loader above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Db-YnYWNRIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photo_dataset = ImageFolder('photos/', transformer)\n",
        "len_training_set = math.floor(len(photo_dataset) * 0.9)\n",
        "len_valid_set = len(photo_dataset) - len_training_set\n",
        "training_set, validation_set = random_split(photo_dataset, (len_training_set, len_valid_set))\n",
        "photo_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)\n",
        "photo_dataloader_valid = DataLoader(validation_set, batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWreH4XwNplc",
        "colab_type": "text"
      },
      "source": [
        "#### show examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIO325gNNqIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_sample_image(photo_dataloader_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhuxp1xAq4CK",
        "colab_type": "text"
      },
      "source": [
        "## Setup tensorboard\n",
        "\n",
        "Use tensorboard to have an eye on weights and losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYjbil6vrsnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/data/My\\ Drive/cartoonGAN/tensorboard/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NqCxs_PrL1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "tensorboard_logdir = '/content/data/My Drive/cartoonGAN/tensorboard'\n",
        "writer = SummaryWriter(tensorboard_logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp40ZnyTc99G",
        "colab_type": "text"
      },
      "source": [
        "##Define model\n",
        "\n",
        "The information about the model structure is given in the paper.\n",
        "\n",
        "For the `zero padding` of the convolutional layers, I use the following formula:\n",
        "\n",
        "$$Height x Width_{output} = \\frac{HeightxWidth_{input} - kernel size + 2 padding}{stride} + 1$$\n",
        "\n",
        "e.g:\n",
        "\n",
        "- `conv_1` layer of generator: $HxW$ should stay the same as input size, which is 256x256 and `stride = 1`\n",
        "\n",
        "$$256 = \\frac{256-7+2padding}{1}+1$$\n",
        "\n",
        "$$padding = 3$$\n",
        "\n",
        "In case of a fraction as a result, I choose to ceil:\n",
        "\n",
        "- `conv_2` layer of generator: $\\frac{H}{2} x \\frac{W}{2}$ is output with `stride=2`\n",
        "\n",
        "$$128 = \\frac{256-3+2padding}{2}+1$$\n",
        "\n",
        "$$padding= \\frac{1}{2} \\Rightarrow padding=1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mW5STOdDyn",
        "colab_type": "text"
      },
      "source": [
        "###Generator\n",
        "\n",
        "In the up-convolutional part of the paper, two layers (conv_6 and conv_8 in my implementation) have a stride of 0.5. As pytorchs `conv2D()` does not allow floating point stride, I use a stride of 1 in both cases. Therefore even for the padding calculation I go with a stride of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaZy5t4508Tz",
        "colab_type": "text"
      },
      "source": [
        "##### Learnings\n",
        "\n",
        "After implementing the generator and getting some results out of the training, the generated images have a wide grey margin, like in this example image:\n",
        "\n",
        "![failure image](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/grey_margin.jpg)\n",
        "\n",
        "I re-checked my implementation of the generator and stumbled across my interpretation of the stride for _conv_6_ and _conv_8_, which is _1/2_ in the paper.\n",
        "Maybe I got the part of the stride wrong, and this is not _0.5_, but a tuple of (1,2)?\n",
        "Then, I did a wrong padding calculation. This are the only layers where I calculated a very large padding of 33 and 65, which looks suspicious now.\n",
        "Testing this, I also ended up with very high values for the padding.\n",
        "\n",
        "The next problem was, that I used _Conv2d_ for up-sampling, but _Conv2d_ is for down-sampling. _ConvTranspose2d_ is for up-sampling, see [1]. I corrected my implementation accordingly.\n",
        "\n",
        "By using _ConvTranspose2d_ with the values for _stride_ (1 or (1,2)) and _kernelsize_ and playing with _padding_, the resulting image keeps nearly the same dimension, shrinks or gets uneven dimensions. \n",
        "\n",
        "$$ HeightxWidth_{Output} = stride (HeightxWidth_{Input} - 1) + kernelsize - 2*padding$$\n",
        "\n",
        "$$ HeightxWidth_{Output} = 1 * (64 - 1) + 3 - 2 * padding = 66 - 2 * padding = \\bigg\\{^{66, p = 0}_{<0, p < 0}$$\n",
        "\n",
        "Uneven dimensions: I tested stride (1,2) with padding (3,2) and got 60x125.\n",
        "\n",
        "But as mentioned in the paper, I need to scale from _H/4_ up to _H/2_, which is from 64 to 128, and then up to 256 in _conv_8_ and _conv_9_.\n",
        "\n",
        "Therefore I decided to use _stride=2_ and _padding=1_ in _conv_6_, and _stride=2_ and _padding_=1_ in _conv_8_. To add the last pixel, I add an _output_padding_ of 1.\n",
        "\n",
        "_conv_6_: $2*(64-1)+3-(2*1)=127 + 1$ _(outer_padding)_\n",
        "\n",
        "_conv_8_: $2*(128-1)+3-(2*1)=255 + 1$ _(outer_padding)_\n",
        "\n",
        "My implementation differs from the paper in the mentioned layers.\n",
        "\n",
        "[1] https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0\n",
        "\n",
        "[2] https://towardsdatascience.com/is-the-transposed-convolution-layer-and-convolution-layer-the-same-thing-8655b751c3a1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2EmDnsucENx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import sigmoid\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv_1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "    self.norm_1 = nn.BatchNorm2d(256)\n",
        "    self.norm_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.norm_2(self.conv_2(F.relu(self.norm_1(self.conv_1(x)))))\n",
        "    return output + x #ES\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Generator, self).__init__()\n",
        "      self.conv_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
        "      self.norm_1 = nn.BatchNorm2d(64)\n",
        "      \n",
        "      # down-convolution #\n",
        "      self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "      self.conv_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_2 = nn.BatchNorm2d(128)\n",
        "      \n",
        "      self.conv_4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
        "      self.conv_5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_3 = nn.BatchNorm2d(256)\n",
        "      \n",
        "      # residual blocks #\n",
        "      residualBlocks = []\n",
        "      for l in range(8):\n",
        "        residualBlocks.append(ResidualBlock())\n",
        "      self.res = nn.Sequential(*residualBlocks)\n",
        "      \n",
        "      # up-convolution #\n",
        "      self.conv_6 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "      self.conv_7 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_4 = nn.BatchNorm2d(128)\n",
        "\n",
        "      self.conv_8 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "      self.conv_9 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "      self.norm_5 = nn.BatchNorm2d(64)\n",
        "      \n",
        "      self.conv_10 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.norm_1(self.conv_1(x)))\n",
        "      \n",
        "      x = F.relu(self.norm_2(self.conv_3(self.conv_2(x))))\n",
        "      x = F.relu(self.norm_3(self.conv_5(self.conv_4(x))))\n",
        "      \n",
        "      x = self.res(x)\n",
        "      x = F.relu(self.norm_4(self.conv_7(self.conv_6(x))))\n",
        "      x = F.relu(self.norm_5(self.conv_9(self.conv_8(x))))\n",
        "\n",
        "      x = self.conv_10(x)\n",
        "\n",
        "      x = sigmoid(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATqXFxcpz0eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator()\n",
        "print(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjtvIkqrVy34",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxJ2dldGz5Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "     super(Discriminator, self).__init__()\n",
        "     self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "      \n",
        "     self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "     self.conv_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_1 = nn.BatchNorm2d(128)\n",
        "      \n",
        "     self.conv_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "     self.conv_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_2 = nn.BatchNorm2d(256)\n",
        "    \n",
        "     self.conv_6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "     self.norm_3 = nn.BatchNorm2d(256)\n",
        "    \n",
        "     self.conv_7 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.conv_1(x))\n",
        "    x = F.leaky_relu(self.norm_1(self.conv_3(F.leaky_relu(self.conv_2(x)))), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.norm_2(self.conv_5(F.leaky_relu(self.conv_4(x)))), negative_slope=0.2)\n",
        "    x = F.leaky_relu(self.norm_3(self.conv_6(x)), negative_slope=0.2)\n",
        "    x = self.conv_7(x)\n",
        "    x = sigmoid(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI6tJdLmlUHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = Discriminator()\n",
        "print(D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwVzSn9nB1jL",
        "colab_type": "text"
      },
      "source": [
        "### use device CPU or GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IMrqAe1lVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(\"Train on GPU.\")\n",
        "else:\n",
        "  print(\"No cuda available\")\n",
        "\n",
        "G.to(device)\n",
        "D.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIU4f6DiCldX",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "$\\mathcal{L}(G, D) = \\mathcal{L}_{adv}(G, D) + ω\\mathcal{L}_{con}(G, D)$ with ω = 10\n",
        "\n",
        "This loss is used to train the discriminator and the generator. In the adversarial part, the discriminator tries to classify the generated images as fakes.\n",
        "\n",
        "During the generator training, the generator tries to minimize the classifications, where the discriminator classifies the generated image as fake. The generator has only affect on the parts of the formula where $G()$ is involved, so the generator tries to minimize this part. Additionally, the loss is not directly calculated from the generator output, but from the discriminator output. Due to the fact that the generator output is the input for the discriminator output in the generator training, the generator is in the chain of the backpropagation, when the loss from the discriminator output is backprogagated all the way back through the discriminator model and generator model to the photo image input data [1], [2].\n",
        "\n",
        "[1] https://developers.google.com/machine-learning/gan/generator\n",
        "\n",
        "[2] https://towardsdatascience.com/only-numpy-implementing-gan-general-adversarial-networks-and-adam-optimizer-using-numpy-with-2a7e4e032021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXuzfGx4Ctlc",
        "colab_type": "text"
      },
      "source": [
        "### Adversarial loss\n",
        "\n",
        "The adversarial loss  $\\mathcal{L}_{adv}(G, D)$ which drives the generator to transform photo to comic style of the image. Its value indicates if the output looks like a cartoon image or not. The paper highlights, that a characteristic part of cartoons images are the clear edges, which are a small detail of the image, must be preserved to generate clear edges in the result. In the paper, this is solved by training not only with cartoon images but additionaly by training with the same cartoon images with smoothed edges so that the discriminator can distinguish between clear and smooth edges. For achieving this the authors define the edge-promoting adversarial loss function:\n",
        "\n",
        "$\\mathcal{L}_{adv}(G, D) = \\mathbb{E}_{ci∼S_{data}(c)}[log D(c_i)]\n",
        "+ \\mathbb{E}_{ej∼S_{data}(e)}[log(1 − D(e_j))]\n",
        "+ \\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$\n",
        "\n",
        "- for the discriminator, this is the formula for the loss function, because output of the Discriminator plays no role within the content loss part of the loss function.\n",
        "\n",
        "- for the initialization phase of the generator, this part of the formula is not used as described in the paper.\n",
        "\n",
        "- for the training phase of the generator, only the part of the formula is used within the generator loss function, which the generator can affect: $\\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEloi8fChEfH",
        "colab_type": "text"
      },
      "source": [
        "### Content loss\n",
        "The content loss $ω\\mathcal{L}_{con}(G, D)$ which preserves the semantic content during transformation. To calculate this, in the paper the high-level feature maps of the VGG network is used, in particular the layer ($l$) `conv4_4`. The output of the layer $l$ for the original photo is subtracted from the output of the layer $l$ of the generated image. The result is regularized using the $\\mathcal{L_1}$ spare regularization ($||...||_1$):\n",
        "\n",
        "$\\mathcal{L}_{con}(G, D)= \\mathbb{E}_{pi~S_{data}(p)}[||VGG_l(G(p_i))-VGG_l(p_i)||_1]$\n",
        "\n",
        "This part of the formula plays a role in the loss function for the generator, not for the discriminator, because only the generator is used within this formula.\n",
        "\n",
        "More info about $\\mathcal{L_1}$ regularization:\n",
        "\n",
        "https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a\n",
        "\n",
        "https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edVjF0YerHgM",
        "colab_type": "text"
      },
      "source": [
        "### VGG-16\n",
        "Load already downloaded vgg-16 weights from drive, or download and save to drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGwo1goXCW_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "path_to_pretrained_vgg16 = '/content/data/My Drive/cartoonGAN/vgg16-397923af.pth'\n",
        "\n",
        "try:\n",
        "  pretrained = torch.load(path_to_pretrained_vgg16)\n",
        "  vgg16 = models.vgg16(pretrained=False)\n",
        "  vgg16.load_state_dict(pretrained)\n",
        "  vgg16 = vgg16.to(device)\n",
        "except FileNotFoundError:\n",
        "  vgg16 = models.vgg16(pretrained=True)\n",
        "  torch.save(vgg16, path_to_pretrained_vgg16)\n",
        "\n",
        "print(vgg16)\n",
        "\n",
        "# due VGG16 has 5 pooling-layer, I assume conv4_4 is the 4th pooling layer\n",
        "# (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "feature_extractor = vgg16.features[:24]\n",
        "for param in feature_extractor.parameters():\n",
        "  param.require_grad = False\n",
        "\n",
        "print(feature_extractor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McDAYOutQ0Xh",
        "colab_type": "text"
      },
      "source": [
        "### Two loss functions\n",
        "\n",
        "- discriminator loss\n",
        "- generator initialization phase loss and generator loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLWoiKZkqKBh",
        "colab_type": "text"
      },
      "source": [
        "##### Learnings\n",
        "\n",
        "At this section I will describe my learnings during implementing/testing the loss functions.\n",
        "\n",
        "At my first implementation, I took the output of the discriminator with size _batch_size x 1 x 64 x 64_ as input to the discriminator loss (to be precise, all three outputs of D, from D(cartoon_image), D(smoothed_cartoon_image) and D(G(photo))).\n",
        "\n",
        "As the adversarial loss outputs a probability which indicates if the input is detected as fraud or not, it returns a single value. To reach this, I took the input tensor with shape _batch_size x 1 x 64 x 64_, and implemented the loss function \n",
        "\n",
        "$\\mathcal{L}_{adv}(G, D) = \\mathbb{E}_{ci∼S_{data}(c)}[log D(c_i)]\n",
        "+ \\mathbb{E}_{ej∼S_{data}(e)}[log(1 − D(e_j))]\n",
        "+ \\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$\n",
        "\n",
        "manually as \n",
        "\n",
        "$torch.log(torch.abs(D(...)) + torch.log(torch.abs(1 - D(...)) + torch.log(torch.abs(1 - D(...))$\n",
        "\n",
        "As the discriminator output sometimes contains negative values, calling _log()_ directly with this value causes an error. Therefore I wrapped _abs()_ around the input of _log()_.\n",
        "\n",
        "As my training results weren't as expected, I came back to the loss functions. As an adversarial loss outputs a probability, thus a single value, my discriminator outputs a tensor with shape _batch_size x 1 x 64 x 64_.\n",
        "\n",
        "So to get a probability out of this tensor, either an activation function is needed, or I made a mistake in implementing the discriminator and it should output a probability. After thinking about this, I went to use an activation function. To reach this, I can either activate the output of the last layer of the discriminator, or I can use a loss function like BCEWithLogitsLoss, which combines activation function and loss.\n",
        "\n",
        "But which activation function to use?\n",
        "\n",
        "As the discriminator should give a probability and only has two classes as outputs, _real_ or _fake_, using sigmoid or softmax is a good choice. Softmax can be used for binary classification as well as classification of _n_-classes.\n",
        "\n",
        "First, I decided to use a loss function, which combines activation and loss function, and this gave me the choice between:\n",
        "\n",
        "- _BCEWithLogitsLoss_: Sigmoid and binary cross entropy loss\n",
        "\n",
        "- _CrossEntroyLoss_: Softmax and negative log likelihood loss\n",
        "\n",
        "For solving a minimax-problem, which loss to choose?\n",
        "\n",
        "\"_If [minimax] implemented directly, this would require changes be made to model weights using stochastic ascent rather than stochastic descent.\n",
        "It is more commonly implemented as a traditional binary classification problem with labels 0 and 1 for generated and real images respectively._\" see [1].\n",
        "\n",
        "Therefore I choosed _BCEWithLogitsLoss_.\n",
        "\n",
        "As _BCEWithLogitsLoss_ has two parameters, one for the input and one for the target, I used _BCEWithLogitsLoss_ three times, one for every different input, and added the values up.\n",
        "\n",
        "But, after trying to go with this solution, the generator produces values lower than zero. This lead to problems when trying to map these values to RGB. Therefore I decide to not combine activation and loss function, and use sigmoid in the generator as well as in the discriminator directly and use _BCELoss_ as loss function.\n",
        "\n",
        "[1] https://machinelearningmastery.com/generative-adversarial-network-loss-functions/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjkXoxfaRI5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "from torch.nn import BCELoss\n",
        "\n",
        "class DiscriminatorLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(DiscriminatorLoss, self).__init__()\n",
        "      self.bce_loss = BCELoss()\n",
        "\n",
        "  def forward(self, discriminator_output_of_cartoon_input,\n",
        "              discriminator_output_of_cartoon_smoothed_input,\n",
        "              discriminator_output_of_generated_image_input,\n",
        "              epoch,\n",
        "              write_to_tensorboard=False):\n",
        "\n",
        "    return self._adversarial_loss(discriminator_output_of_cartoon_input,\n",
        "                     discriminator_output_of_cartoon_smoothed_input,\n",
        "                     discriminator_output_of_generated_image_input,\n",
        "                     epoch,\n",
        "                     write_to_tensorboard)\n",
        "\n",
        "  def _adversarial_loss(self, discriminator_output_of_cartoon_input,\n",
        "                     discriminator_output_of_cartoon_smoothed_input,\n",
        "                     discriminator_output_of_generated_image_input,\n",
        "                     epoch,\n",
        "                     write_to_tensorboard):\n",
        "\n",
        "    # define ones and zeros here instead within __init__ due to have same shape as input\n",
        "    # due to testing different batch_sizes, sometimes the \"last batch\" has < batch_size elements\n",
        "    actual_batch_size = discriminator_output_of_cartoon_input.size()[0]\n",
        "    zeros = torch.zeros([actual_batch_size, 1, 64, 64]).to(device)\n",
        "    ones = torch.ones([actual_batch_size, 1, 64, 64]).to(device)\n",
        "\n",
        "    d_loss_cartoon = self.bce_loss(discriminator_output_of_cartoon_input, ones)\n",
        "    d_loss_cartoon_smoothed = self.bce_loss(discriminator_output_of_cartoon_smoothed_input, zeros)\n",
        "    d_loss_generated_input = self.bce_loss(discriminator_output_of_generated_image_input, zeros)\n",
        "\n",
        "    d_loss = d_loss_cartoon + d_loss_cartoon_smoothed + d_loss_generated_input\n",
        "\n",
        "    if write_to_tensorboard:\n",
        "      writer.add_scalar('d_loss_cartoon', d_loss_cartoon,epoch)\n",
        "      writer.add_scalar('d_loss_cartoon_smoothed', d_loss_cartoon_smoothed, epoch)\n",
        "      writer.add_scalar('d_loss_generated_input', d_loss_generated_input, epoch)\n",
        "      writer.add_scalar('d_loss', d_loss, epoch)\n",
        "\n",
        "    return d_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw0l1TRkimY_",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter omega ($\\omega$)\n",
        "\n",
        "Initially, I set $\\omega$, which is a weight to balance the style and the content preservation, to the value given in the paper, which is 10. After running 210 epochs, the content preservation was very good, but the generated images do not have cartoon styles. Maybe this is a problem with my input data, where I use different cartoon styles from different artists instead from one single artist, as used in the paper.\n",
        "\n",
        "Examine the parts of the generator loss over time, the following values are observable:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<img src=\"https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/g_content_loss.png\" alt=\"g_content_loss\" width=\"400\">\n",
        "</td><td>\n",
        "<img src=\"https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/g_adversarial_loss.png\" alt=\"g_content_loss\" width=\"400\">\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "So the content loss is magnitudes higher than the adversarial loss.\n",
        "Maybe my calculation of the content loss is wrong? Should it be much lower? As the generated images preserve the content very good, I concentrate on the adversarial loss.\n",
        "\n",
        "As the adversarial loss is responsible for the comic-effect, I try a much lower $\\omega$, to balance the values of g_content_loss and g_adversarial_loss on an equal level.\n",
        "\n",
        "As g_content_loss has values of $4e+5$, I choose $\\omega=0.00001$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-JzisW-65IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      super(GeneratorLoss, self).__init__()\n",
        "      self.w = 0.00001\n",
        "      self.bce_loss = BCELoss()\n",
        "      self.feature_extractor = vgg16.features[:24]\n",
        "      for param in self.feature_extractor.parameters():\n",
        "        param.require_grad = False\n",
        "\n",
        "  def forward(self, discriminator_output_of_generated_image_input,\n",
        "              generator_input,\n",
        "              generator_output,\n",
        "              epoch,\n",
        "              is_init_phase=False,\n",
        "              write_to_tensorboard=False):\n",
        "    if is_init_phase:\n",
        "      g_content_loss = self._content_loss(generator_input, generator_output)\n",
        "      g_adversarial_loss = 0.0\n",
        "      g_loss = g_content_loss\n",
        "    else:\n",
        "      g_adversarial_loss = self._adversarial_loss_generator_part_only(discriminator_output_of_generated_image_input)\n",
        "      g_content_loss = self._content_loss(generator_input, generator_output)\n",
        "      g_loss = g_adversarial_loss + self.w * g_content_loss\n",
        "\n",
        "    if write_to_tensorboard:\n",
        "      writer.add_scalar('g_adversarial_loss', g_adversarial_loss, epoch)\n",
        "      writer.add_scalar('g_content_loss', g_content_loss, epoch)\n",
        "      writer.add_scalar('g_loss', g_loss, epoch)\n",
        "\n",
        "    return g_loss\n",
        "\n",
        "  def _adversarial_loss_generator_part_only(self, discriminator_output_of_generated_image_input):\n",
        "    actual_batch_size = discriminator_output_of_generated_image_input.size()[0]\n",
        "    ones = torch.ones([actual_batch_size, 1, 64, 64]).to(device)\n",
        "    return self.bce_loss(discriminator_output_of_generated_image_input, ones)\n",
        "\n",
        "  def _content_loss(self, generator_input, generator_output):\n",
        "    return (self.feature_extractor(generator_output) - self.feature_extractor(generator_input)).norm(p=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAlrFRkH2WaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminatorLoss = DiscriminatorLoss()\n",
        "generatorLoss = GeneratorLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF8stH-X3e7j",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer\n",
        "In the paper, the used optimizer is not mentioned, I decide to choose adam.\n",
        "\n",
        "For hyperparameter-tuning, I decided to go with the same parameters mentioned in the DCGAN-paper[1].\n",
        "\n",
        "[1] https://arxiv.org/pdf/1511.06434.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwrrY1z34WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
        "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGP0eIqibdAN",
        "colab_type": "text"
      },
      "source": [
        "## Saving\n",
        "To make training resumeable, I save some checkpoints to google drive and load them, if existing, before run the training.\n",
        "\n",
        "I also save weights and bias from generator and discriminator to tensorboard.\n",
        "\n",
        "For checking some intermediate images of the generator, I save them to google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9b23aS94msx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/data/My\\ Drive/cartoonGAN/checkpoints/\n",
        "!mkdir -p /content/data/My\\ Drive/cartoonGAN/intermediate_results/training/\n",
        "intermediate_results_training_path = \"/content/data/My Drive/cartoonGAN/intermediate_results/training/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4zSMmblNxWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_training_result(input, output):\n",
        "  # input/output has batch-size number of images, get first one and detach from tensor\n",
        "  image_input = input[0].detach().cpu().numpy()\n",
        "  image_output = output[0].detach().cpu().numpy()\n",
        "  # transponse image from torch.Size([3, 256, 256]) to (256, 256, 3)\n",
        "  image_input = np.transpose(image_input, (1, 2, 0))\n",
        "  image_output = np.transpose(image_output, (1, 2, 0))\n",
        "\n",
        "  # generate filenames as timestamp, this orders the output by time\n",
        "  filename = str(int(time.time()))\n",
        "  path_input = intermediate_results_training_path + filename + \"_input.jpg\"\n",
        "  path_output = intermediate_results_training_path + filename + \".jpg\"\n",
        "  plt.imsave(path_input, image_input)\n",
        "  plt.imsave(path_output, image_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJYC1ec5ye3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_model_weights_and_bias_to_tensorboard(prefix, state_dict, epoch):\n",
        "  for param in state_dict:\n",
        "      writer.add_histogram(f\"{prefix}_{param}\", state_dict[param], epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6weYKGsHB-Vo",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoxZv-93DEXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(_num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses):\n",
        "  init_epochs = 10\n",
        "  print_every = 100\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(_num_epochs - epochs_already_done):\n",
        "    epoch = epoch + epochs_already_done\n",
        "\n",
        "    for index, ((photo_images, _), (smoothed_cartoon_images, _), (cartoon_images, _)) in enumerate(zip(photo_dataloader_train, smoothed_cartoon_image_dataloader_train, cartoon_image_dataloader_train)):\n",
        "      batch_size = photo_images.size(0)\n",
        "      photo_images = photo_images.to(device)\n",
        "      smoothed_cartoon_images = smoothed_cartoon_images.to(device)\n",
        "      cartoon_images = cartoon_images.to(device)\n",
        "\n",
        "      # train the discriminator\n",
        "      d_optimizer.zero_grad()\n",
        "      \n",
        "      d_of_cartoon_input = D(cartoon_images)\n",
        "      d_of_cartoon_smoothed_input = D(smoothed_cartoon_images)\n",
        "      d_of_generated_image_input = D(G(photo_images))\n",
        "\n",
        "      write_only_one_loss_from_epoch_not_every_batch_loss = (index == 0)\n",
        "\n",
        "      d_loss = discriminatorLoss(d_of_cartoon_input,\n",
        "                                 d_of_cartoon_smoothed_input,\n",
        "                                 d_of_generated_image_input,\n",
        "                                 epoch,\n",
        "                                 write_to_tensorboard=write_only_one_loss_from_epoch_not_every_batch_loss)\n",
        "\n",
        "      d_loss.backward()\n",
        "      d_optimizer.step()\n",
        "\n",
        "      # train the generator\n",
        "      g_optimizer.zero_grad()\n",
        "\n",
        "      g_output = G(photo_images)\n",
        "      #save some intermediate results during training\n",
        "      if (index % 10) == 0:\n",
        "        save_training_result(photo_images, g_output)\n",
        "\n",
        "      d_of_generated_image_input = D(g_output)\n",
        "\n",
        "      if epoch < init_epochs:\n",
        "        # init\n",
        "        init_phase = True\n",
        "      else:\n",
        "        # train\n",
        "        init_phase = False\n",
        "\n",
        "      g_loss = generatorLoss(d_of_generated_image_input,\n",
        "                              photo_images,\n",
        "                              g_output,\n",
        "                              epoch,\n",
        "                              is_init_phase=init_phase,\n",
        "                              write_to_tensorboard=write_only_one_loss_from_epoch_not_every_batch_loss)\n",
        "\n",
        "      g_loss.backward()\n",
        "      g_optimizer.step()\n",
        "\n",
        "      if (index % print_every) == 0:\n",
        "        losses.append((d_loss.item(), g_loss.item()))\n",
        "        now = time.time()\n",
        "        current_run_time = now - start_time\n",
        "        start_time = now\n",
        "        print(\"Epoch {}/{} | d_loss {:6.4f} | g_loss {:6.4f} | time {:2.0f}s | total no. of losses {}\".format(epoch+1, _num_epochs, d_loss.item(), g_loss.item(), current_run_time, len(losses)))\n",
        "    \n",
        "    # write to tensorboard\n",
        "      write_model_weights_and_bias_to_tensorboard('D', D.state_dict(), epoch)\n",
        "      write_model_weights_and_bias_to_tensorboard('G', G.state_dict(), epoch)\n",
        "\n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "      D.eval()\n",
        "      G.eval()\n",
        "\n",
        "      for batch_index, (photo_images, _) in enumerate(photo_dataloader_valid):\n",
        "        photo_images = photo_images.to(device)\n",
        "\n",
        "        g_output = G(photo_images)\n",
        "        d_of_generated_image_input = D(g_output)\n",
        "        g_valid_loss = generatorLoss(d_of_generated_image_input,\n",
        "                                      photo_images,\n",
        "                                      g_output,\n",
        "                                      epoch,\n",
        "                                      is_init_phase=init_phase,\n",
        "                                      write_to_tensorboard=write_only_one_loss_from_epoch_not_every_batch_loss)\n",
        "\n",
        "        if batch_index % print_every == 0:\n",
        "          validation_losses.append(g_valid_loss.item())\n",
        "          now = time.time()\n",
        "          current_run_time = now - start_time\n",
        "          start_time = now\n",
        "          print(\"Epoch {}/{} | validation loss {:6.4f} | time {:2.0f}s | total no. of losses {}\".format(epoch+1, _num_epochs, g_valid_loss.item(), current_run_time, len(validation_losses)))\n",
        "\n",
        "    D.train()\n",
        "    G.train()\n",
        "    \n",
        "    if(g_valid_loss.item() < best_valid_loss):\n",
        "      print(\"Generator loss improved from {} to {}\".format(best_valid_loss, g_valid_loss.item()))\n",
        "      best_valid_loss = g_valid_loss.item()\n",
        "  \n",
        "    # save checkpoint\n",
        "    checkpoint = {'g_valid_loss': g_valid_loss.item(),\n",
        "                  'best_valid_loss': best_valid_loss,\n",
        "                  'losses': losses,\n",
        "                  'validation_losses': validation_losses,\n",
        "                  'last_epoch': epoch+1,\n",
        "                  'd_state_dict': D.state_dict(),\n",
        "                  'g_state_dict': G.state_dict(),\n",
        "                  'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                  'g_optimizer_state_dict': g_optimizer.state_dict()\n",
        "                }\n",
        "    print(\"Save checkpoint for validation loss of {}\".format(g_valid_loss.item()))\n",
        "    torch.save(checkpoint, checkpoint_dir + '/checkpoint_epoch_{:03d}.pth'.format(epoch+1))\n",
        "    if(best_valid_loss == g_valid_loss.item()):\n",
        "      print(\"Overwrite best checkpoint\")\n",
        "      torch.save(checkpoint, checkpoint_dir + '/best_checkpoint.pth')\n",
        "    \n",
        "  return losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P6_f7NTDXyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "checkpoint_dir = '/content/data/My Drive/cartoonGAN/checkpoints'\n",
        "checkpoints = listdir(checkpoint_dir)\n",
        "num_epochs = 200 + 10 # training + init phase\n",
        "epochs_already_done = 0\n",
        "best_valid_loss = math.inf\n",
        "losses = []\n",
        "validation_losses = []\n",
        "\n",
        "if(len(checkpoints) > 0):\n",
        "  last_checkpoint = sorted(checkpoints)[-1]\n",
        "  checkpoint = torch.load(checkpoint_dir + '/' + last_checkpoint, map_location=torch.device(device))\n",
        "  best_valid_loss = checkpoint['best_valid_loss']\n",
        "  epochs_already_done = checkpoint['last_epoch']\n",
        "  losses = checkpoint['losses']\n",
        "  validation_losses = checkpoint['validation_losses']\n",
        "  \n",
        "  D.load_state_dict(checkpoint['d_state_dict'])\n",
        "  G.load_state_dict(checkpoint['g_state_dict'])\n",
        "  d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "  g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "  print('Load checkpoint {} with g_valid_loss {}, best_valid_loss {}, {} epochs and total no of losses {}'.format(last_checkpoint, checkpoint['g_valid_loss'], best_valid_loss, epochs_already_done, len(losses)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppzLtQGwBBAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses, validation_losses = train(num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VxBiLKImhYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure.format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_losses = [x[0] for x in losses]\n",
        "g_losses = [x[1] for x in losses]\n",
        "plt.plot(d_losses, label='Discriminator training loss')\n",
        "plt.plot(g_losses, label='Generator training loss')\n",
        "plt.plot(validation_losses, label='Generator validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR4iOWxP7ZK4",
        "colab_type": "text"
      },
      "source": [
        "### Show results in tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7QbonJE7WcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='/content/data/My Drive/cartoonGAN/tensorboard'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qz2e1MbOYg",
        "colab_type": "text"
      },
      "source": [
        "### Plot losses\n",
        "\n",
        "Losses after 210 epochs ![losses_after_210_epochs](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/losses_after_210_epochs.png)\n",
        "\n",
        "Epoch 210/210 | d_loss 0.0000 | g_loss 615573.0625 | time 194s | total no. of losses 648\n",
        "\n",
        "Epoch 210/210 | validation loss 552576.4375 | time 45s | total no. of losses 216"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW3m4tOObhSx",
        "colab_type": "text"
      },
      "source": [
        "### some generated results\n",
        "\n",
        "#### direct after start, one of the first epochs\n",
        "\n",
        "![some of the first epochs](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/no_margin.jpg)\n",
        "\n",
        "#### direct after init-phase is completed\n",
        "**Epoch 10/210 | d_loss 0.0124 | g_loss 226864.062**\n",
        "\n",
        "**Epoch 10/210 | validation loss 182869.5312**\n",
        "\n",
        "Photo input ![epoch_10_photo_input](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_10_photo_input.jpg) Generated image ![epoch_10_generated_image](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_10_generated_image.jpg)\n",
        "\n",
        "#### direct at the beginning of epoch 11 with use of full generator loss instead of init loss. These results seem to be outliers at this stage of training due to the next outputs look more similar like the inputs\n",
        "\n",
        "Photo input ![epoch_11_photo_input](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_11_photo_input.jpg) Generated image ![epoch_11_generated_image](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_11_generated_image.jpg)\n",
        "\n",
        "Photo input ![epoch_11_photo_input](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_11_photo_input_2.jpg) Generated image ![epoch_11_generated_image](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/epoch_11_generated_image_2.jpg)\n",
        "\n",
        "#### Epoch 210\n",
        "\n",
        "After training has finished 210 epochs, the output looks like this:\n",
        "\n",
        "\n",
        "Photo input ![input_210_epochs](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/input_210_epochs.jpg) Generated image ![generated_after_210_epochs.jpg](https://github.com/TobiasSunderdiek/cartoon-gan/raw/master/assets/generated_after_210_epochs.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ4wLFRfr0dS",
        "colab_type": "text"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4bnUSksAiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load(checkpoint_dir + '/best_checkpoint.pth')\n",
        "G_inference = Generator()\n",
        "G_inference.load_state_dict(checkpoint['g_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0nUlPZIALF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images = iter(photo_dataloader_valid).next()[0]\n",
        "result_images = G_inference(test_images)\n",
        "print(result_images[0])\n",
        "plt.imshow(np.transpose(result_images[0].detach().numpy(), (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1njcrDnS_Me",
        "colab_type": "text"
      },
      "source": [
        "## Notes/next steps\n",
        "- alternative lib for image processing: https://github.com/albu/albumentations\n",
        "- figure out which variant of VGG to use (VGG-16?), and if the pre-training in the referenced paper is the same as the pre-trained pytorch version\n",
        "- do I use correct normalization-method in content loss\n",
        "- in which order is the discriminator trained regarding photos, cartoons with smoothed edges and then genereated images?\n",
        "- evaluate result with existing model http://cg.cs.tsinghua.edu.cn/people/~Yongjin/CartoonGAN-Models.rar ?\n",
        "- did I split the loss function correctly for the D and G model, and content loss only for G?\n",
        "- plot results directly from vars via method\n",
        "- in the paper 6.000 photo images and 2.000 - 4.000 cartoon images are used for training, how is this done with unbalanced datasets?\n",
        "- is batch_size of 16 correct? Tried 32 before, but got CUDA OOM\n",
        "- for image downloader: catch exception if image is truncated/check if zipping adds additional folder within .zip in create_smoothed_images.py\n",
        "- upgrade to tensorboard 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUUiJBAjqea9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}